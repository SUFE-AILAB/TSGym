# TSGym

## Design Dimensions
### Series Normalization
- None
- Statistic
- RevIN

### Series Decomposition
- None
- Seasonal + Trend

### Series Patching 
- Use or not

### Input Encoding
- Linear projection (non-Transformer based)
- Linear projection + Positional Encoding (Transformer based)
- Series Patching (only for long-term forecasting task)

### Network Architecture

## Attention
- Self-attention
- Auto-Correlation
- Sparse Attention
- Frequency Enhanced Attention

## todo
- 20241211: TransformerGym_None_series-patching_sparse-attention, loss全是0? (nan)